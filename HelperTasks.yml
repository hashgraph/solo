version: 3
output: prefixed
silent: false
vars:
  nodes:
    ref: until (env "SOLO_NETWORK_SIZE" | default .SOLO_NETWORK_SIZE | int)
  # node name should be node1, node2, node3, etc.
  node_list_internal: "{{range $idx, $n := .nodes }}node{{add $n 1}},{{end}}"
  node_identifiers: "{{ .node_list_internal | trimSuffix \",\" }}"

  solo_user_dir: "{{ .solo_home_override_dir | default (printf \"%s/.solo\" (env \"HOME\")) }}"
  solo_cache_dir: "{{ .solo_user_dir }}/cache"
  solo_logs_dir: "{{ .solo_user_dir }}/logs"
  solo_keys_dir: "{{ .solo_cache_dir }}/keys"
  solo_bin_dir: "{{ .solo_user_dir }}/bin"
  run_build_file:
    sh: (echo "/tmp/run-build-$(date +%Y%m%d%H%M%S)")
  var_check_file:
    sh: (echo "/tmp/var-check-$(date +%Y%m%d%H%M%S)")
  minio_flag_file:
    sh: (echo "/tmp/minio-flag-$(date +%Y%m%d%H%M%S)")

  # TODO: test local build path
  # TODO: make port forwards optional, doesn't work in Alex's multiple users on the same machine setup

env:
  SOLO_CLUSTER_SETUP_NAMESPACE: solo-setup
  SOLO_CLUSTER_RELEASE_NAME: solo-cluster-setup
  SOLO_CLUSTER_NAME: solo-cluster
  MIRROR_RELEASE_NAME: mirror

tasks:
  init:
    cmds:
      - task: "var:check"
      - task: "run:build"

  var:check:
    silent: true
    status:
      - test -f {{ .var_check_file }}
    requires:
      vars:
        - solo_user_dir
        - solo_cache_dir
        - solo_logs_dir
        - solo_keys_dir
        - solo_bin_dir
        - nodes
        - node_list_internal
        - node_identifiers
        - run_build_file
        - SOLO_CHART_VERSION
        - CONSENSUS_NODE_VERSION
        - SOLO_NAMESPACE
        - SOLO_CLUSTER_SETUP_NAMESPACE
        - SOLO_CLUSTER_RELEASE_NAME
        - SOLO_NETWORK_SIZE
        - SOLO_CLUSTER_NAME
        - MIRROR_RELEASE_NAME
    cmds:
      - echo "Checking variables..."
      - echo "solo_user_dir={{ .solo_user_dir }}"
      - echo "SOLO_HOME=${SOLO_HOME}"
      - echo "SOLO_NETWORK_SIZE=${SOLO_NETWORK_SIZE}"
      - echo "SOLO_CHART_VERSION=${SOLO_CHART_VERSION}"
      - echo "CONSENSUS_NODE_VERSION=${CONSENSUS_NODE_VERSION}"
      - echo "SOLO_NAMESPACE=${SOLO_NAMESPACE}"
      - echo "nodes={{ .nodes }}"
      - echo "node_identifiers={{ .node_identifiers }}"
      - echo "VALUES_FLAG=${VALUES_FLAG}"
      - echo "SETTINGS_FLAG=${SETTINGS_FLAG}"
      - echo "LOG4J2_FLAG=${LOG4J2_FLAG}"
      - echo "APPLICATION_PROPERTIES_FLAG=${APPLICATION_PROPERTIES_FLAG}"
      - echo "LOCAL_BUILD_FLAG=${LOCAL_BUILD_FLAG}"
      - touch {{ .var_check_file }}

  readme:
    silent: true
    cmds:
      - echo "This is a custom network configuration for the Hedera Hashgraph Solo network."
      - echo "The network is configured to have {{ .SOLO_NETWORK_SIZE }} nodes."
      - echo "The network is deployed in the namespace {{ .SOLO_NAMESPACE }}."
      - echo "The cluster is deployed in the namespace {{ .SOLO_CLUSTER_SETUP_NAMESPACE }}."
      - echo "Use command 'task default' to deploy the network."
      - echo "Use command 'task destroy' to destroy the network."
      - echo "Use command 'task clean' to destroy and clean up the network."
      - echo "Use command 'task show:ips' to show the external IPs of the nodes."
      - echo "Use command 'task default-with-mirror' to deploy the network with a mirror node."
      - echo "Use command 'task default-with-relay' to deploy the network with a relay node."

  install:solo:
    internal: true
    cmds:
      - cd ..
      - npm install

  install:kubectl:darwin:
    internal: true
    platforms:
      - darwin
    status:
      - command -v kubectl
    cmds:
      - brew update
      - brew install kubernetes-cli

  install:kubectl:linux:
    internal: true
    platforms:
      - linux
    status:
      - command -v kubectl
    cmds:
      - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/{{ ARCH }}/kubectl"
      - sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
      - rm -rf kubectl

  solo:init:
    internal: true
    silent: true
    deps:
      - task: "init"
    status:
      - test -f {{ .solo_bin_dir }}/helm
      - test -f {{ .solo_cache_dir }}/profiles/custom-spec.yaml
      - test -f {{ .solo_cache_dir }}/templates/api-permission.properties
      - test -f {{ .solo_cache_dir }}/templates/application.properties
      - test -f {{ .solo_cache_dir }}/templates/bootstrap.properties
      - test -f {{ .solo_cache_dir }}/templates/settings.txt
      - test -f {{ .solo_cache_dir }}/templates/log4j2.xml
      #- test "$(yq -r '.flags."node-ids"' < {{ .solo_user_dir }}/solo.yaml)" == "{{ .node_identifiers }}"
      - test "$(jq -r '.flags."node-ids"' < {{ .solo_user_dir }}/solo.config)" == "{{ .node_identifiers }}"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- init

  solo:keys:
    internal: true
    status:
      - |
        for n in $(seq 0 {{ sub (env "SOLO_NETWORK_SIZE" | default .SOLO_NETWORK_SIZE | int) 1 }}); do
         test -f {{ .solo_keys_dir }}/hedera-node${n}.crt
         test -f {{ .solo_keys_dir }}/hedera-node${n}.key
         test -f {{ .solo_keys_dir }}/s-public-node${n}.pem
         test -f {{ .solo_keys_dir }}/s-private-node${n}.pem
        done
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- node keys --gossip-keys --tls-keys --node-aliases-unparsed {{.node_identifiers}} -q

  solo:network:deploy:
    internal: true
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- network deploy --namespace "${SOLO_NAMESPACE}" --node-aliases-unparsed {{.node_identifiers}} --release-tag "${CONSENSUS_NODE_VERSION}" --solo-chart-version "${SOLO_CHART_VERSION}" ${VALUES_FLAG} ${SETTINGS_FLAG}  ${LOG4J2_FLAG} ${APPLICATION_PROPERTIES_FLAG} -q
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- node setup --namespace "${SOLO_NAMESPACE}" --node-aliases-unparsed {{.node_identifiers}} --release-tag "${CONSENSUS_NODE_VERSION}" ${LOCAL_BUILD_FLAG} -q

  solo:network:destroy:
    internal: true
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- network destroy --namespace "${SOLO_NAMESPACE}" --delete-pvcs --delete-secrets --force -q

  solo:node:start:
    internal: true
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- node start --namespace "${SOLO_NAMESPACE}" --node-aliases-unparsed {{.node_identifiers}} -q {{ .CLI_ARGS }}
      - kubectl port-forward -n "${SOLO_NAMESPACE}" svc/haproxy-node1-svc 50211:50211 &
      - task: "sleep_after_port_forward"

  solo:node:stop:
    internal: true
    ignore_error: true
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- node stop --namespace "${SOLO_NAMESPACE}" --node-aliases-unparsed {{.node_identifiers}} -q {{ .CLI_ARGS }}

  solo:relay:
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- relay deploy -n "${SOLO_NAMESPACE}" -i node1 -q
      - echo "Enable port forwarding for Hedera JSON RPC Relay"
      - kubectl port-forward -n "${SOLO_NAMESPACE}" svc/relay-node1-hedera-json-rpc-relay 7546:7546 &
      - task: "sleep_after_port_forward"

  solo:destroy-relay:
    status:
      - helm list -n "${SOLO_NAMESPACE}" | grep -vqz relay-node1
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- relay destroy -n "${SOLO_NAMESPACE}" -i node1 -q

  solo:cache:remove:
    internal: true
    status:
      - test [[ ! -d {{ .solo_cache_dir }} ]]
    cmds:
      - rm -rf {{ .solo_cache_dir }}

  solo:logs:remove:
    internal: true
    status:
      - test [[ ! -d {{ .solo_logs_dir }} ]]
    cmds:
      - rm -rf {{ .solo_logs_dir }}

  solo:config:remove:
    internal: true
    status:
      - test [[ ! -f {{ .solo_user_dir }}/solo.yaml ]]
    cmds:
      - rm -rf {{ .solo_user_dir }}/solo.yaml

  cluster:create:
    status:
      - kind get clusters | grep -q "${SOLO_CLUSTER_NAME}"
    cmds:
      - kind create cluster -n "${SOLO_CLUSTER_NAME}" --image "${KIND_IMAGE}"
      - sleep 10 # wait for control plane to come up

  cluster:setup:
    deps:
      - task: "init"
    cmds:
      - SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- cluster setup --cluster-setup-namespace "${SOLO_CLUSTER_SETUP_NAMESPACE}" -q

  cluster:destroy:
    cmds:
      - kind delete cluster --name "${SOLO_CLUSTER_NAME}"

  clean:port-forward:
    cmds:
      - pkill -f "kubectl port-forward -n {{ .SOLO_NAMESPACE }}" | grep ${UID} || true

  sleep_after_port_forward:
    cmds:
      # somehow without the sleep, when port-forward is the last command of a series of tasks, port-forward
      # prematurely killed when task is exiting
      - sleep 4

  run:build:
    silent: true
    status:
      - test -f {{ .run_build_file }}
    cmds:
      - npm run build
      - touch {{ .run_build_file }}

  solo:cluster:minio:
    internal: true
    silent: true
    cmds:
      - |
        if ! kubectl get svc -l app.kubernetes.io/instance=minio-operator --all-namespaces --no-headers | grep -q . ; then
          echo "No services found with label app.kubernetes.io/name=operator app.kubernetes.io/instance=minio-operator"
          echo "--minio" > {{ .minio_flag_file }}
        else 
          echo "--no-minio" > {{ .minio_flag_file }}
        fi

  solo:cluster:setup:
    silent: true
    deps:
      - task: "init"
      - task: "solo:cluster:minio"
    status:
      - helm list --all-namespaces | grep -qz "${SOLO_CLUSTER_RELEASE_NAME}"
    cmds:
      - |
        export MINIO_FLAG=$(cat {{ .minio_flag_file }})
        SOLO_HOME_DIR=${SOLO_HOME_DIR} npm run solo -- cluster setup --cluster-setup-namespace "${SOLO_CLUSTER_SETUP_NAMESPACE}" ${MINIO_FLAG} -q

  solo:node:addresses:
    internal: true
    cmds:
      - kubectl get svc -n "${SOLO_NAMESPACE}" -l "solo.hedera.com/type=network-node-svc" --output=go-template-file={{ .ip_list_template_file }}

  start:
    desc: solo node start
    deps:
      - task: "init"
    cmds:
      - task: "solo:node:start"

  stop:
    desc: solo node stop
    deps:
      - task: "init"
    cmds:
      - task: "solo:node:stop"

  show:ips:
    deps:
      - task: "init"
    cmds:
      - task: "solo:node:addresses"

  clean:cache:
    desc: remove solo cache directory
    deps:
      - task: "init"
    cmds:
      - task: "solo:cache:remove"

  clean:logs:
    desc: remove solo logs director
    deps:
      - task: "init"
    cmds:
      - task: "solo:logs:remove"
